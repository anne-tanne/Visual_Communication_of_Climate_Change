\subsection{DALL-E 3 API for Image Generation}
\label{subsec:dalle-api}

\textit{DALL-E} is now accessible through an API by \textit{OpenAI}, allowing developers to integrate its capabilities directly into their applications.

Below is an example of how to use the \textit{DALL-E} API in Python to generate and download an image. This code snippet demonstrates how to programmatically request \textit{DALL-E} to create an image based on a specified prompt and then download the generated image to a local directory.
\begin{lstlisting}
import requests
import os

api_key = 'given-api-key-here'

def download_image(url, save_path, file_name):
    """
    Downloads an image from the specified URL and saves it to a local directory.
    """
    # Send a GET request to the URL
    response = requests.get(url)
    if response.status_code == 200:
        # Create the save directory if it doesn't exist
        os.makedirs(save_path, exist_ok=True)
        # Create the full path for the new file
        file_path = os.path.join(save_path, file_name)
        # Open the file and write the image content to it
        with open(file_path, 'wb') as file:
            file.write(response.content)
        print(f"Image downloaded successfully: {file_path}")
        return file_path
    else:
        # Raise an exception if there's an error in downloading
        raise Exception(f"Error downloading image: {response.status_code}")

def generate_image(prompt, model="dall-e-3", size="1024x1024", quality="standard", n=1, api_key = api_key):
    """
    Generates an image based on a text prompt using the DALL-E API.
    """
    # Set up the headers for the API request
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }

    # Define the data payload for the POST request
    data = {
        "model": model,
        "prompt": prompt,
        "size": size,
        "quality": quality,
        "n": n
    }

    # Make the POST request to the DALL-E API
    response = requests.post("https://api.openai.com/v1/images/generations", json=data, headers=headers)
    if response.status_code == 200:
        # Return the URL of the generated image
        return response.json()['data'][0]['url']
    else:
        # Raise an exception if there's an error in the API call
        raise Exception(f"Error in API call: {response.status_code} {response.text}")

def generate_and_download_image(prompt, save_path, file_name):
    """
    Combines the functions of generating an image and downloading it.
    """
    # Generate the image and retrieve the URL
    image_url = generate_image(prompt)
    # Download the image from the URL
    return download_image(image_url, save_path, file_name)

# Example usage
prompt = "a photorealistic image of a tsunami"
save_path = "downloads"
file_name = "tsunami.jpg"
generate_and_download_image(prompt, save_path, file_name)


\end{lstlisting}

\subsection{\textit{Midjourney} Image Generation Using Default Interface}
\label{subsec:midjourney-default-interface}

In this study, the images are generated using \textit{Midjourney}'s standard interface, specifically version 6.0 of the model. This approach was chosen instead of using third-party APIs such as \textit{ImagineAPI.dev} in order to comply with legal and ethical standards.

The process of generating images in \textit{Midjourney} involves manually entering specific prompts directly into the \textit{Midjourney} interface on \textit{Discord}. An example of such a prompt is illustrated below:
\begin{verbatim}
/imagine prompt: a photorealistic image of a tsunami --v 6 --s 100
\end{verbatim}
This example demonstrates the format and parameters used:

\begin{itemize}
\item \textbf{Prompt Command:} The \texttt{/imagine} command initiates the image generation process.
\item \textbf{Prompt Content:} The actual prompt, in this case, "\textit{a photorealistic image of a tsunami}" defines the subject of the generated image.
\item \textbf{Version Parameter (\texttt{--v}):} The \texttt{--v 6} parameter specifies the use of \textit{Midjourney} version 6.0.
\item \textbf{Stylize Parameter (\texttt{--s}):} The \texttt{--s 100} parameter sets the level of stylisation. Low stylisation values produce images that are close to the prompt but less artistic. High stylisation values produce images that are very artistic but less in line with the prompt.  The default value of \texttt{--stylize} is 100 while the parameter accepts integer values from 0-1000 when the current model is used.
\end{itemize}

\subsection{Image Naming Convention and Organisation}

A specific naming and storage convention will be used to systematically manage the generated images, where each image will be named according to the format \texttt{ImageID\_PromptID\_Model\_Date\_PromptExcerpt}. This structure makes it easier to track and identify the images in the data set.

\begin{itemize}
    \item \texttt{ImageID}: A unique identifier for each image.
    \item \texttt{PromptID}: The identifier of the prompt used, which links each image to its source prompt.
    \item \texttt{Model}: The AI model used for generation (\textit{DALL-E} or \textit{Midjourney}).
    \item \texttt{Date}: The date the image was created, which helps with chronological analysis.
    \item \texttt{PromptExcerpt}: An excerpt from the prompt for a quick context reference.
\end{itemize}

The images will be stored in a root directory, with individual subfolders for each prompt.

\subsection{Visual Attribute Detection}
\label{subsec:visual-attribute-detection}

This Python script is designed to analyse images and extract key visual attributes, specifically average colour, brightness, and saturation. The script performs the following steps:

\begin{enumerate}
    \item \textbf{Reading the Image}: The image is read in BGR (Blue, Green, Red) format using OpenCV.
    \item \textbf{Colour Conversion}: The image is then converted to RGB (Red, Green, Blue) format for colour analysis, and to HSV (Hue, Saturation, Value) format for brightness and saturation analysis.
    \item \textbf{Calculating Averages}: It computes the average colour in the RGB format, and the average brightness and saturation from the HSV format.
    \item \textbf{Output}: The script returns the calculated averages of colour, brightness, and saturation for further analysis.
\end{enumerate}

\begin{lstlisting}
import matplotlib.pyplot as plt
import numpy as np
import cv2

# Function to analyse an image and extract its visual attributes
def analyse_image(image_path):
    # Read the image in BGR format
    image = cv2.imread(image_path)
    
    # Convert the image to RGB format
    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    
    # Convert the image to HSV format for brightness and saturation analysis
    image_hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
    
    # Calculate the average color (in RGB)
    average_colour = np.mean(image_rgb, axis=(0, 1))
    
    # Round the average color values and convert to integer
    rounded_average_colour = np.round(average_colour).astype(int)

    # Calculate the overall average intensity of RGB
    average_rgb_intensity = np.mean(rounded_average_colour)

    # Calculate the average brightness (V channel of HSV)
    average_brightness = np.mean(image_hsv[:, :, 2])
    
    # Calculate the average saturation (S channel of HSV)
    average_saturation = np.mean(image_hsv[:, :, 1])
    
    # Display the image
    plt.imshow(image_rgb)
    plt.axis('off')
    plt.title('Analysed Image')
    plt.show()
        
    # Return the computed visual attributes
    return rounded_average_colour, average_rgb_intensity, average_brightness, average_saturation
\end{lstlisting}

\subsection{Intercoder- and Intracoder-Reliability}
\label{subsec:reliability-code-script}

This Python script is designed to calculate Krippendorff's alpha for two given coding matrices. The function will return a dictionary with Krippendorff's alpha scores for each column that is present in both dataframes.

\begin{lstlisting}
import krippendorff
import numpy as np

def calculate_krippendorff_alpha_for_all_columns(df1, df2):
    alpha_scores = {}
    # Iterate over columns that are present in both dataframes and are numeric
    for col in df1.columns.intersection(df2.columns):
        if pd.api.types.is_numeric_dtype(df1[col]) and pd.api.types.is_numeric_dtype(df2[col]) and col != 'CU':
            # Stack the two coders' ratings together for the current column
            ratings = np.vstack([df1[col].values, df2[col].values])
            
            # Calculate Krippendorff's alpha for the current column, handling NaN values
            alpha = krippendorff.alpha(reliability_data=ratings, level_of_measurement='nominal')
            alpha_scores[col] = alpha
    return alpha_scores

\end{lstlisting}