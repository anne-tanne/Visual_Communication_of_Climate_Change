\section{Methods}

The study focuses on the image-to-text models \textit{DALL-E 3} and \textit{Midjourney v6} due to their popularity and advanced capabilities.

The reason for choosing these models lies in their widespread public discussion in the field of AI image generation \parencite{Gibson2023, Gindham2023, Guiness2023}.

\textit{DALL-E 3} was recently integrated into \textit{Microsoft Bing}, which offers simple access to the model \parencite{Bing2024}. The model's direct connection to \textit{ChatGPT} further enhances its public visibility. \textit{Midjourney}, while less accessible due to its \textit{Discord} interface and discontinuation of its free trial in April 2023 \parencite{Weiss2023}, still boasts over 16 million users \parencite{Krivec2023}. As of January 2024, \textit{Midjourney} lacks a first-party API.

Another notable model, \textit{Stable Diffusion}, provides free access and a first-party API, presenting some advantages over \textit{Midjourney}. However, it lags behind in computing efficiency \parencite{Kothari2023} and user interest compared to \textit{Midjourney} and \textit{DALL-E} (see Figure \ref{fig:google_search_trends}), justifying their selection for the study.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{resources/google_search_trends.png}
    \caption[Text-to-Image Models Google Search Trends]{Worldwide Google Search Trends Over Time}
    \label{fig:google_search_trends}
\end{figure}

\subsection{Designing Prompts}
\label{subsec:designing-prompts}

For this study, 27 prompts were crafted, focusing on extreme weather events and their various aspects. These prompts, detailed in the appendix (see Appendix Chapter \ref{appendix:prompts}), were designed to balance guidance and interpretative freedom, ensuring sufficient room for interpretation to ensure a wide range of responses. The aim was to provide open-ended prompts to some degree, but specific enough to generate images that were relevant and insightful in the context of extreme weather events. These prompts are mainly based on the the literature review (see Chapter \ref{subsec:climate-change-and-extreme-weather}).

The initial prompt ($n=1$) was broad ("\textit{Depict an extreme weather event}"), allowing for diverse interpretations and representations of extreme weather.

Building on the general prompt, a series of more specific prompts ($n=8$) were developed, each targeting a specific type of extreme weather event as categorised by the IPCC (see table \ref{tab:extreme_weather_events}). This includes prompts such as "\textit{Depict temperature extremes}". These prompts aim to analyse how the models represent different weather phenomena.

Another four prompts ($n=4$) aim to explore the visualisation of the impacts of these events on life, landscapes, infrastructure, and cultural changes, informed by a literature review. Additionally, two prompts ($n=2$) focus on recovery and adaptation strategies. Finally, prompts were developed focusing on the causes of extreme weather events ($n=3$) and future climate scenarios ($n=9$). These prompts aim to identify the models' understandings of the causes of extreme weather events and their visualisation of future weather patterns and climate change impacts.

\subsubsection{Image Generation}
The prompts designed for this study (as described in \ref{subsec:designing-prompts}) will be submitted to the official \textit{DALL-E 3} API, while for  \textit{Midjourney} the official \textit{Discord} interface will be used for data generation.

\noindent
For generating images using the \textit{DALL-E 3} API, the following parameters were chosen:

\begin{itemize}
\item \textbf{Model Specification:}'\texttt{dall-e-3}' to access \textit{DALL-E}'s latest features.
\item \textbf{Image Size:} Uniform resolution of '1024x1024'.
\item \textbf{Image Quality:} "\texttt{standard}" setting for balanced fidelity and speed.
\item \textbf{Number of images:} '\texttt{n}' set to 1, as per \textit{DALL-E 3}'s one-image-per-call limit.
\end{itemize}

\noindent An example of how this API is used can be found in the appendix (see Appendix Chapter \ref{subsec:dalle-api}).
\vspace{4mm}

For the creation of images with \textit{Midjourney}, the standard \textit{Discord} interface will be used. The decision was motivated by the absence of an official API and potential legal and ethical implications associated with third-party APIs.

\begin{itemize}
\item \textbf{Model Version:} \textit{Midjourney v6.0} ( \texttt{--v 6.0}) for latest features.
\item \textbf{Stylize Parameter:}  The \texttt{--stylize} parameter influences the application of \textit{Midjourney}'s default artistic style. Low values produce images closer to the prompt but less artistic, while high values yield very artistic but less prompt-aligned images \parencite{Midjourney}. The default and used value for this study is 100, and the parameter accepts integer values ranging from 0 to 1000.
\item \textbf{Image Upscaling:} All images will be upscaled post-generation.
\end{itemize}

\subsection{Sample}

This study will analyse a total of 240 images, generated from 27 (+3) unique prompts with each prompt producing eight images (four from \textit{DALL-E} and four from \textit{Midjourney}). \textit{Midjourney} generates four images per prompt by default, while \textit{DALL-E} will receive each prompt four times.

The reason for creating four images per prompt is twofold: First, to capture the variability of AI interpretations, as the same model can often visualise the same prompt differently, leading to a variety of results. Second, while the first images tend to show a large variety, this tends to decrease with subsequent images (see Figure \ref{fig:ai_outputs})

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{resources/comparison_output_numbers.png}
\caption[AI Model Image Variability]{Comparison of the output for a single prompt by \textit{DALL-E 3}. The first four images show diverse interpretations, while the last four indicate reduced variability.}
\label{fig:ai_outputs}
\end{figure}

\subsection{Content Analysis}
\label{subsec:manual-content-analysis}

The content analysis is an essential part of this study and follows on from the collection of material as described above. 

\subsubsection{Codebook}
\label{subsubsec:codebook}

The core of a content analysis lies in the development of a category scheme (codebook), which is necessary in order to make statements about the collected material.

The codebook contains both formal categories and content categories. Each variable defined in the codebook is accompanied by a set of coding instructions that detail the conditions under which coders should assign particular codes to the images. 

\subsubsection{Exclusion Criteria}
The study's general exclusion criteria omits certain AI-generated images to maintain focus. Excluded are off-topic images clearly unrelated to prompts, images focused on text due to limitations of the models to generate coherent text within images, and images with technical flaws like poor rendering or low resolution.

\subsubsection{Formal Categories}
\label{subsubsec:formal-categories}
It was determined that only the visual content of an image would be analysed. Each image will be assigned a unique identifier, the Coding Unit (CU), which serves as a reference throughout the analysis. Information such as the model, or the prompt used will be recorded for each CU.

\paragraph{Narrative Constructs}
\label{subsubsec:content-categories}
This section is primarily focused on analysing how AI-generated images depict and contextualise extreme weather events. It examines the overall storytelling approach, including how these events are visually framed within different environmental settings, the emotional and thematic undertones conveyed, and the portrayal of impacts and potential responses.

\paragraph{Visual Attributes}
\label{subsubsec:coding-visual-attributes}
These variables assess visual attributes such as colour, brightness, and saturation. A script (see Appendix, p. \pageref{subsec:visual-attribute-detection})  will be employed to extract these attributes from each image computationally.

\paragraph{Visual Synechdoches}
This section focuses on discerning symbolic imagery and metaphors that encapsulate broader narratives and themes related to climate change.

\subsubsection{Coding Process}
\label{subsubsec:coding-process}

\paragraph{Creation of a Coding Matrix}
A coding matrix was created in Microsoft Excel to facilitate coding. It reflects the categories of the codebook and enforces the validity of the data through restrictions, including drop-down lists to limit entries to predefined codes and prevent invalid data entries.
 
\paragraph{Training and Reliability}
Coders will be trained with sample images to ensure a shared understanding of coding instructions. Before the actual coding, a pre-test phase is carried out to assess the clarity of the coding book and refine the coding rules if necessary. Intra-coder reliability is also monitored to confirm individual coders' consistency over time. The Krippendorff's alpha statistical measure will be used to assess consistency, aiming for a reliability coefficient of $0.8$ (see Appendix Chapter \ref{subsec:reliability-code-script}).

\subsection{Statistical Analysis}
Descriptive statistics will serve as a base, especially for RQ1 and RQ3, which aim to categorise and quantify  different narrative themes and elements depicted. An ANOVA is used on RQ2 to compare mean visual attributes (such as colour, brightness and saturation) across different image categories to determine whether certain attributes are more consistent with anxiety-inducing images. Furthermore, correlation and (mulitnominal logistic) regression analyses will be conducted to examine and potentially predict relationships between the type of extreme weather event depicted and various image attributes. For RQ4 chi-square tests are used to determine whether the distributions of narratives and visual elements differ significantly between the models.